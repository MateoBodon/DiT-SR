trainer:
  target: trainer.TrainerDifIR

four_channel_autoencoder:
  ckpt_path: ./weights/your_4channel_autoencoderkl.pth
  trainable: false
  compile: false
  params:
    embed_dim: 4
    ddconfig:
      double_z: True
      z_channels: 4
      resolution: 256
      in_channels: 4
      out_ch: 4
      ch: 128
      ch_mult: [1, 2]
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0

model:
  target: models.unet.DiTSRModel
  ckpt_path: ~
  params:
    image_size: 64
    in_channels: 3
    model_channels: 160
    out_channels: 3
    attention_resolutions: [64, 32, 16, 8]
    dropout: 0.1
    channel_mult: [1, 2, 2, 4]
    conv_resample: True
    dims: 2
    use_fp16: False
    num_head_channels: 32
    swin_depth: 6
    swin_embed_dim: 192
    window_size: 8
    mlp_ratio: 4
    cond_lq: True
    lq_size: 64

diffusion:
  target: models.script_util.create_gaussian_diffusion
  params:
    sf: 4
    schedule_name: exponential
    schedule_kwargs:
      power: 0.3
    etas_end: 0.99
    steps: 15
    min_noise_level: 0.04
    kappa: 2.0
    weighted_mse: False
    predict_type: xstart
    timestep_respacing: ~
    scale_factor: 1.0
    normalize_input: True
    latent_flag: True

autoencoder:
  target: ldm.models.autoencoder.VQModelTorch
  ckpt_path: weights/autoencoder_vq_f4.pth
  use_fp16: True
  params:
    embed_dim: 3
    n_embed: 8192
    ddconfig:
      double_z: False
      z_channels: 3
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [1, 2, 4]
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
      padding_mode: zeros

degradation:
  sf: 4
  resize_prob: [0.2, 0.7, 0.1]
  resize_range: [0.15, 1.5]
  gaussian_noise_prob: 0.5
  noise_range: [1, 30]
  poisson_scale_range: [0.05, 3.0]
  gray_noise_prob: 0.4
  jpeg_range: [30, 95]
  second_order_prob: 0.5
  second_blur_prob: 0.8
  resize_prob2: [0.3, 0.4, 0.3]
  resize_range2: [0.3, 1.2]
  gaussian_noise_prob2: 0.5
  noise_range2: [1, 25]
  poisson_scale_range2: [0.05, 2.5]
  gray_noise_prob2: 0.4
  jpeg_range2: [30, 95]
  gt_size: 256
  resize_back: False
  use_sharp: False

data:
  train:
    name: SEN2NAIP_Train
    type: SEN2NAIPDataset # Name of your new class
    dataroot_gt: /path/to/your/sen2naip_dataset/train/HR_NAIP_patches_256x256/
    dataroot_lq: /path/to/your/sen2naip_dataset/train/LR_S2like_patches_64x64/
    # Or if using a meta file:
    # meta_info_file: /path/to/your/sen2naip_train_meta.txt 
    # dataroot_gt: /path_prefix_for_gt_in_meta_file/ (can be empty if paths in meta are absolute)
    # dataroot_lq: /path_prefix_for_lq_in_meta_file/
    filename_tmpl: '{}' # If scanning folders and files are directly named, e.g. 0001.tif
    io_backend: {type: disk} # For local file system

    gt_size: 256      # HR patch size for training
    scale: 4          # Upscaling factor
    use_hflip: true
    use_rot: true

    num_worker_per_gpu: 4 
    batch_size_per_gpu: 2 # Start small with 4-channel data
    dataset_enlarge_ratio: 1
    phase: train

  val:
    name: SEN2NAIP_Val
    type: SEN2NAIPDataset
    dataroot_gt: /path/to/your/sen2naip_dataset/val/HR_NAIP_patches/
    dataroot_lq: /path/to/your/sen2naip_dataset/val/LR_S2like_patches/
    # meta_info_file: /path/to/your/sen2naip_val_meta.txt
    io_backend: {type: disk}
    scale: 4
    phase: val # No random crop/augmentation usually for validation
    gt_size: 256 # Validation patches might be loaded as is or center-cropped

train:
  lr: 5e-5
  lr_min: 2e-5
  lr_schedule: ~
  warmup_iterations: 5000
  batch: [64, 8]
  microbatch: 8
  num_workers: 4
  prefetch_factor: 2
  weight_decay: 0
  ema_rate: 0.999
  iterations: 300000
  save_freq: 10000
  log_freq: [200, 2000, 1]
  local_logging: True
  tf_logging: False
  use_ema_val: True
  val_freq: ${train.save_freq}
  val_y_channel: True
  val_resolution: ${model.params.lq_size}
  val_padding_mode: reflect
  use_amp: True
  seed: 123456
  global_seeding: False
  compile:
    flag: False
    mode: reduce-overhead