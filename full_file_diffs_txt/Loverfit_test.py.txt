diff --git a/overfit_test.py b/overfit_test.py
new file mode 100644
index 0000000..9e4304f
--- /dev/null
+++ b/overfit_test.py
@@ -0,0 +1,67 @@
+import yaml
+import torch
+from omegaconf import OmegaConf
+
+# Import the Trainer and the Dataset
+from trainer import TrainerDifIR
+from datapipe.sen2naip_dataset import SEN2NAIPDataset
+import os
+
+# --- A simplified logger for this test ---
+class SimpleLogger:
+    def info(self, msg):
+        print(msg)
+
+def main():
+    # --- 1. Load Configuration ---
+    config_path = './configs/realsr_DiT.yaml'
+    print("--- Loading Configuration ---")
+    with open(config_path, 'r') as f:
+        yaml_config = yaml.safe_load(f)
+    configs = OmegaConf.create(yaml_config)
+
+    # --- 2. Build the Trainer and Models ---
+    # We will piggyback on the Trainer class as it handles all the setup
+    print("\n--- Building Trainer, Model, and Optimizer ---")
+    
+    # The Trainer class expects a logger attribute
+    trainer = TrainerDifIR(configs)
+    trainer.logger = SimpleLogger() 
+    
+    # Build the DiT model and the VAE autoencoder
+    trainer.build_model() 
+    
+    # Set up the AdamW optimizer
+    trainer.setup_optimizaton() 
+
+    # --- 3. Get a Single Batch of Data ---
+    print("\n--- Preparing a Single Batch of Data for Overfitting ---")
+    dataset = SEN2NAIPDataset(configs.data.train)
+    # We only need one batch. Batch size is defined in your YAML.
+    dataloader = torch.utils.data.DataLoader(dataset, batch_size=configs.train.microbatch, shuffle=True) 
+    
+    # Get one batch and move it to the GPU
+    the_only_batch = next(iter(dataloader))
+    the_only_batch = trainer.prepare_data(the_only_batch)
+    print("Data batch is ready.")
+
+    # --- 4. The Overfitting Loop ---
+    print("\n--- Starting Overfitting Test (Loss should decrease rapidly) ---")
+    trainer.model.train() # Put model in training mode
+    
+    for i in range(1, 301): # Let's run for 300 iterations
+        # Always use the exact same batch of data
+        losses, _, _ = trainer.training_step(the_only_batch)
+        
+        if i % 10 == 0: # Print the loss every 10 steps
+            # The 'l1' loss is the one we care about
+            print(f"Iteration {i:03d} | Loss: {losses['l1'].mean().item():.6f}")
+
+    print("\n--- Overfitting Test Complete ---")
+    print("If the loss value above steadily decreased towards zero, your model and training loop are working correctly!")
+
+
+if __name__ == '__main__':
+    # Set a dummy rank environment variable for the Trainer class
+    os.environ['LOCAL_RANK'] = '0' 
+    main()
\ No newline at end of file
